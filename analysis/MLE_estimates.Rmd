---
title: "MLE"
author: "Margarita Orlova"
date: "November 15, 2019"
output: html_document
---
# MLE estimates and sampling variability

In this section we infer parameter estimates by maximum likelihood and estimate sampling variability of MLE. 

As explained in previous data simulation section, mRNA gene expression can be modelled as three or four parameter model.

For three-parameter model probability distribution is:

$P(X\leq x) = \int_0^{1} P(X\leq x|k_r*p)f(p)dp=\frac{1}{B(k_{on},k_{off})}\int_0^{1} P(X\leq x|k_r*p)p^{k_{on}-1}(1-p)^{k_{off}-1}dp$

Using $t=2p-1$, $p=\frac{t+1}{2}$ and $dt=2dp$ we have

$\frac{1}{B(k_{on},k_{off})}\frac{1}{2^{k_{on}+k_{off}-1}}\int_{-1}^{1} P(X\leq x|\frac{k_r(t+1)}{2})(1+t)^{k_{on}-1}(1-t)^{k_{off}-1}$, the integral can be computed with the use of Gauss-Jacobi quadrature method.

And as we discussed previously, $BetaPoisson(x|k_{on},k_{off},k_r,d) = BetaPoisson(\frac{x}{d}|k_{on},k_{off},k_r)$, so same applies to the probability distribution of four-parameter model.

Using pdf of the model we can calculate log-likelihood for the data  $logL(k_{on},k_{off},k_{r},d)=\sum_x n(x) log(p(\frac{x}{d}|k_{on},k_{off},k_{r}))$, where $n(x)$ is the number of cells with mRNA expression level equal to x.

To optimize the computations, we will choose initial values using method of moments.

E(X) and Var(X) can be estimated from the data directly 


$Var(X|k_r*p)=E(X|k_r*p)=k_r*p$ as mean and variance of Poisson distribution

$E(X)=E(E(X|k_r*p))=k_r*\frac{k_{on}}{k_{on}+k_{off}}$ by Tower Law

$Var(X)=E(Var(X|k_r*p))+Var(E(X|k_r*p))=E(k_r*p) + Var(k_r*p) = k_r\frac{k_{on}}{k_{on}+k_{off}} + k_r^2\frac{k_{on}*k_{off}}{(k_{on}+k_{off})^2(k_{on}+k_{off}+1)}$ by law of total variance.

From that we can estimate $k_{on}=\frac{E(X)}{k_r}(\frac{E(X)(k_r-E(X))}{Var(X)-E(X)}-1)$ and $k_{off}=k_{on}*\frac{k_r-E(X)}{E(X)}$

d is estimated as 0.1*median(X), if this value is less then 1.0 and 1.0 otherwise. After we choose d, we also scale data prior to other parameter initialization.  Initial estimate of $k_r$ is chosen to be max(X).

Now we will implement the method above, using code provided by Larsson et al. publication.
Since the original code is in python, we have to source it from MLE.py script, generate 1000 datasets with same $k_{on}$,$k_{off}$ and $k_r$ and get the point estimates. We repeat this process 4 times with different $k_{on}$,$k_{off}$ scenarios.

```{r, echo=FALSE}
library(reticulate)
library(robustbase)
source_python("C:/Users/Moonkin/Documents/GitHub/Thesis_single_RNA/analysis/MLE.py")
```


## I) Small $k_{on}$ and $k_{off}$ case:

```{r,echo=FALSE}
#True k_on=k_off=0.1
n_estimates=1000
k_on=as.numeric(0.1)
k_off=as.numeric(0.1)
kr=as.numeric(100)
n_cells=as.integer(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)
}

hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=0.1, log scale", xlab="k_on MLE estimate")
abline(v=log(0.1),col='red')
paste("true k_on is 0.1;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=0.1, log scale", xlab="k_off MLE estimate")
abline(v=log(0.1),col='red')
paste("true k_off is 0.1;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs,true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
points(log(0.1),log(0.1),col="red",pch = 19,font=4)
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
points(log(0.1),log(100),col="red",pch = 19,font=4)
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
points(log(0.1),log(100),col="red",pch = 19,font=4)
```

Small $k_{on}$ and $k_{off}$ leads to bimodal distribution. As we see from histograms, mean and median MLE estimates are centered around true value.

## II) Large $k_{on}$ and $k_{off}$ case: 

```{r,echo=FALSE}
#True k_on=k_off=10
k_on=as.numeric(10)
k_off=as.numeric(10)
kr=as.numeric(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)
}
hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=10, log scale", xlab="k_on MLE estimate")
abline(v=log(10),col='red')
paste("true k_on is 10;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=10, log scale", xlab="k_off MLE estimate")
abline(v=log(10),col='red')
paste("true k_off is 10;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs, true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
points(log(10),log(10),col="red",pch = 19,font=4)
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
points(log(10),log(100),col="red",pch = 19,font=4)
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
points(log(10),log(100),col="red",pch = 19,font=4)
```

Large $k_{on}$ and $k_{off}$ case lead to Poisson or negative binomial distribution and as we can see parameters are not identifiable.  

## III) Large value if $k_{off}$ and small $k_{on}$ case:

```{r, echo=FALSE}
#True k_on=1, k_off=10
k_on=as.numeric(1)
k_off=as.numeric(10)
kr=as.numeric(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)
}

hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=1, log scale", xlab="k_on MLE estimate")
abline(v=log(1),col='red')
paste("true k_on is 1;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=10, log scale", xlab="k_off MLE estimate")
abline(v=log(10),col='red')
paste("true k_off is 10;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs, true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
points(log(1),log(10),col="red",pch = 19,font=4)
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
points(log(1),log(100),col="red",pch = 19,font=4)
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
points(log(10),log(100),col="red",pch = 19,font=4)
```

k_on parameter being estimated correctly, while k_off and k_r are non-identifiable.

## IV) Small value if $k_{off}$ and big $k_{on}$

```{r, echo=FALSE}
#True k_on=10, k_off=1
k_on=as.numeric(10)
k_off=as.numeric(1)
kr=as.numeric(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)}

hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=10, log scale", xlab="k_on MLE estimate")
abline(v=log(10),col='red')
paste("true k_on is 10;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=1, log scale", xlab="k_off MLE estimate")
abline(v=log(1),col='red')
paste("true k_off is 1;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs, true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
points(log(10),log(1),col="red",pch = 19,font=4)
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
points(log(10),log(100),col="red",pch = 19,font=4)
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
points(log(1),log(100),col="red",pch = 19,font=4)
```



Ref.
https://www.nature.com/articles/s41586-018-0836-1
