---
title: "MLE"
author: "Margarita Orlova"
date: "November 15, 2019"
output: html_document
---
# MLE estimates and sampling variability

In this section we infer point estimates $k_{on}$,$k_{off}$ and $k_r$ by maximum likelihood and estimate sampling variability of MLE. 

Now we will implement the method above, using code provided by Larsson et al. publication.
Since the original code is in python, we have to source it from MLE.py script, generate 1000 datasets with same $k_{on}$,$k_{off}$ and $k_r$ and get the point estimates. We repeat this process 4 times with different $k_{on}$,$k_{off}$ scenarios.

```{r, echo=FALSE}
library(reticulate)
library(robustbase)
source_python("C:/Users/Moonkin/Documents/GitHub/Thesis_single_RNA/analysis/MLE.py")
```


## I) Small $k_{on}$ and $k_{off}$ case:

```{r,echo=FALSE}
#True k_on=k_off=0.1
n_estimates=1000
k_on=as.numeric(0.1)
k_off=as.numeric(0.1)
kr=as.numeric(100)
n_cells=as.integer(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)
}

hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=0.1, log scale", xlab="k_on MLE estimate")
abline(v=log(0.1),col='red')
paste("true k_on is 0.1;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=0.1, log scale", xlab="k_off MLE estimate")
abline(v=log(0.1),col='red')
paste("true k_off is 0.1;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs,true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
```

Small $k_{on}$ and $k_{off}$ leads to bimodal distribution. As we see from histograms, mean and median MLE estimates are centered around true value.

## II) Large $k_{on}$ and $k_{off}$ case: 

```{r,echo=FALSE}
#True k_on=k_off=10
k_on=as.numeric(10)
k_off=as.numeric(10)
kr=as.numeric(100)
n_cells=as.integer(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)
}
hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=10, log scale", xlab="k_on MLE estimate")
abline(v=log(10),col='red')
paste("true k_on is 10;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=10, log scale", xlab="k_off MLE estimate")
abline(v=log(10),col='red')
paste("true k_off is 10;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs, true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
```

Large $k_{on}$ and $k_{off}$ case lead to Poisson or negative binomial distribution and as we can see parameters are not identifiable.  

## III) Large value if $k_{off}$ and small $k_{on}$ case:

```{r, echo=FALSE}
#True k_on=1, k_off=10
k_on=as.numeric(1)
k_off=as.numeric(10)
kr=as.numeric(100)
n_cells=as.integer(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)
}

hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=1, log scale", xlab="k_on MLE estimate")
abline(v=log(1),col='red')
paste("true k_on is 1;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=10, log scale", xlab="k_off MLE estimate")
abline(v=log(10),col='red')
paste("true k_off is 10;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs, true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
```

k_on parameter being estimated correctly, while k_off and k_r are non-identifiable.

## IV) Small value if $k_{off}$ and big $k_{on}$

```{r, echo=FALSE}
#True k_on=10, k_off=1
n_estimates=100
k_on=as.numeric(10)
k_off=as.numeric(1)
kr=as.numeric(100)
n_cells=as.integer(100)
est=matrix(, nrow = n_estimates, ncol = 3)
for (i in 1:n_estimates){
  x=generate_data(k_on,k_off,kr,n_cells)
  est[i,]=MaximumLikelihood(x)}

hist(log(est[,1]),breaks=30, main="MLE estimate of k_on parameter,1000 runs, true k_on=10, log scale", xlab="k_on MLE estimate")
abline(v=log(10),col='red')
paste("true k_on is 10;","k_on mean",round(mean(est[,1],na.rm=TRUE),3),"k_on median",round(median(est[,1],na.rm=TRUE),3))

hist(log(est[,2]),breaks=30, main="MLE estimate of k_off parameter,1000 runs, true k_off=1, log scale", xlab="k_off MLE estimate")
abline(v=log(1),col='red')
paste("true k_off is 1;","k_off mean",round(mean(est[,2],na.rm=TRUE),3),"k_off median",round(median(est[,2],na.rm=TRUE),3))

hist(log(est[,3]),breaks=30, main="MLE estimate of k_r parameter,1000 runs, true k_r=100, log scale", xlab="k_r MLE estimate")
abline(v=log(100),col='red')
paste("true k_r is 100;","k_r mean",round(mean(est[,3],na.rm=TRUE),3),"k_r median",round(median(est[,3],na.rm=TRUE),3))

plot(log(est[,1]),log(est[,2]), xlab="k_on",ylab="k_off",main="k_on vs k_off, log scale")
plot(log(est[,1]),log(est[,3]), xlab="k_on",ylab="k_r",main="k_on vs k_r, log scale")
plot(log(est[,2]),log(est[,3]), xlab="k_off",ylab="k_r",main="k_off vs k_r, log scale")
```



Ref.
https://www.nature.com/articles/s41586-018-0836-1
